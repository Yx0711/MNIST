In this project, we use different machine learning methods as well as several neutral network architecture to analyze MNIST dataset.

Load MNIST data from torchvision package and Preprocessing data by divide each element in X_train and X_test by 255 such that the value of each element lines in [0,1], The change Y_train and Y_test to one hot label by using Keras package.

Before deep learning, we tried to use some machine learning model to train the data and reproduce the test error in the paper. First we use KNN with K=10 and get test error=4.42%.Secondly, we use Adaboost with 70 decision trees classifiers with maximum depth 10 and get test error=4.01%. Then, using SVM with C=5 and Gaussian kernel to reproduce the 1.4% error in the paper. Finally, I use 3 times convolutional layer in Keras to get test error =0.89% which is outperform all of three model I mentioned previously.

For the deep learning part, I use pytorch to analyze data. First we put train data in train_loader and test_data in testLoader by using Dataloader in torch package, define function to calculate how many misclassification case we have. then using cross-entropy as criterion and SGD as optimizer to train the data and calculate the misclassification error. In this part, we use single layer model with ReLu as activation function and get the test error=7.53%. And the second model is Conv2d-relu-Maxpool-Conv2d, we choose kernel size=3 and input channel=64, then get the test error=1.25%. Finally, I use my favorite model with 2 convolutional layer and Batch-norm to get the lowest test error which is 0.67%, in this question, I define plot function to plot misclassification error vs epochs and loss error vs epochs. Because this model preform very well, it is enough to use 60 epochs to achieve lower test error and overfitting. All of these three model, we visualized weight by using plt.ishow(W) and tuning hyper-parameter to see how convergence rate change as we increase and decrease learning rate and momentum. We first fixed the momentum=0.0 and try different lr=0.01,0.2,0.5, and then we fixed learning rate =0.1, and momentum = 0.0,0.5,0.9.

Finally, this is a new dataset that the last column is the summation of the two previous digits. We reshape our data as shape 28*56 pixels.Then I try to use Tensorflow to analyze the data. First model is 1 convolutional layer with MaxPooling, and this model is relatively simple, so the test and validation error is relatively higher. After tuning the parameters the lowest validation error is about 13% and test error is about 11.8%. Then I tried a more complicated model with 3 convolutional layer with dropout =0.2 and batch normalization. This time, the performance is better, and we get test error =3%. Redo the part 3c we get filter visualization, and using for loop to see the algorithm performance for different hyper-parameters.